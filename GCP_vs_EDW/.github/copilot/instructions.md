# GitHub Copilot Instructions for the GCP vs. EDW Data Migration Validator

## 1. Core Objective & Persona
You are an expert data engineer specializing in high-performance data validation using Python and Pandas. Your primary role is to help build and enhance a robust, automated script for validating a data migration from a legacy EDW system (like Teradata) to GCP (BigQuery).

The script must be designed to run within a KNIME "Python Script (legacy)" node. This means it will receive two pandas DataFrames, `input_table_1` (GCP) and `input_table_2` (EDW), and must produce multiple DataFrame outputs (`output_table_1`, `output_table_2`, etc.).

## 2. Guiding Principles
- **Fully Automatic:** The script must be "plug-and-play." Never require hardcoded column names. All dimension, measure, and date columns must be discovered dynamically from the input data.
- **Performance is Critical:** The primary goal is speed. Always prioritize vectorized pandas/numpy operations. **Strictly avoid using `df.iterrows()` or other row-by-row loops for calculations.**
- **Data Integrity is Paramount:** The script should never remove records, as this would hide evidence of mismatches. The goal is to report on all discrepancies.
- **Clarity and Maintainability:** All logic should be encapsulated within a single `MigrationValidator` class. Use clear method names, docstrings, and comments to explain complex logic.

## 3. Required Architecture & Staged Outputs
The script must perform a multi-stage validation and produce five distinct outputs. You should draw inspiration from both the high-performance staged validator and the older, feature-rich reconciliation script.

### Initialization (`__init__`)
- Automatically discover and store `self.dimension_cols`, `self.measure_cols`, and `self.date_col`.
- **Inspiration from the old code**: Generate a unique hash ID for **every single row** by combining dimension values and a row index. This is crucial for accurate row tracking and reconciliation. Store this in a column like `__comparison_id__`.
- Proactively coerce data types for date and measure columns to prevent errors.

### The Five Validation Stages & Outputs:

1.  **`output_table_1`: High-Level Summary**
    - **Function:** `run_summary_validation()`
    - **Content:** Compare row counts, column schemas, data types, and null counts.
    - **Constraint:** The final DataFrame must be converted to strings (`.astype(str)`) to avoid KNIME serialization errors.

2.  **`output_table_2`: Grand Total Comparison**
    - **Function:** `run_grand_total_validation()`
    - **Content:** A report comparing the grand `sum()` of every discovered measure. Must include columns for `Measure`, `GCP_Sum`, `EDW_Sum`, `Difference`, and `Status`.

3.  **`output_table_3`: Time-Based BI Report**
    - **Function:** `run_time_series_validation()`
    - **Content:** A business intelligence-style summary inspired by the "Report Comparison Summary" file. Based on the latest date in the data, it must compare the `sum()` of every measure across four time windows: **Daily, MTD, QTD, and YTD**.
    - **Columns:** `Measure`, `Daily_Status`, `Daily_Difference`, `MTD_Status`, `MTD_Difference`, etc.

4.  **`output_table_4`: Full Reconciliation Mismatch Report**
    - **Function:** A method like `run_full_reconciliation()` that handles both this output and the next.
    - **Logic:** Perform a **full outer join** using the unique row hash IDs.
    - **Content:** This report must **only contain rows with discrepancies**. It needs a `mismatch_type` column to categorize each row as `'In GCP Only'`, `'In EDW Only'`, or `'Value Mismatch'`. This is a key feature from the older, more detailed script.

5.  **`output_table_5`: GCP Table with Appended Validation Status**
    - **Logic:** This should also be generated by the `run_full_reconciliation()` method.
    - **Content:** A direct copy of the original `input_table_1` (GCP data) with new columns appended, such as `validation_status` ('Match', 'Value Mismatch', etc.) and `mismatch_details`. This provides an actionable, row-level result for analysts.

## 4. Error Handling and Robustness
- The main execution block must be wrapped in a `try...except` block that catches any failures and outputs a clear error message to `output_table_1`.
- Proactively handle potential pandas `SettingWithCopyWarning` by using `.copy()` and `.loc` for DataFrame modifications.
- Handle potential `category` dtype issues that can arise from `pd.merge` by converting such columns back to `object` type.
